---
title: "ClusterAnalysis_PartitioningMethod"
author: "Ben Renne"
date: "11 11 2020"
output: html_document
---

## Load libraries and set working directory
```{r Load Packages, message = FALSE}
workingDir <- "C:/Users/tamta/Documents/Studium/02_Master/17_Masterarbeit/"

library(magrittr)
library(ecodist)
library(factoextra)
library(tidyverse)
library(cluster)
library(gridExtra)
```

## Create test data
```{r Create Data}
#one dimensional data
alter <- c(43, 38, 6, 47, 37, 9)
#two dimensional data
#data of arrests and population density in the USA.
df <- USArrests %>% na.omit() %>% scale()
head(df)
```

## Perform the Partinioning Cluster Analysis
The function k-means is used with:

- x: matrix of the data
- center: the center and the number of the classes, must be predefined
- nstart: try 25 times, then pick the best

It is important, that the number of classes has to be predefined. But later in this document, a method is presented that helps finding the correct class-number.
```{r Cluster Analysis}
#1D DATA
cluster_alter <- kmeans(x = alter, center = 2)
#additionally, tjhe centers of the respective classes can be defined as well as they are yet unknown, the first two elements are arbitrarily picked
cluster_alter <- kmeans(x = alter, center = alter[1:2])

#distance matrix/full distance matrix
dm <- dist(alter)
dm <- full(dist(alter))

#2D DATA
cluster_df <- kmeans(x = df, center = 4, nstart = 25)

#distance matrix
distance <- dist(df)
```

## Understand the cluster results
The results of the cluster analysis can be called using the *Dollar* sign. For that, the 1D data is used.  
The mean of each class is called by **$center**.
```{r Mean}
cluster_alter$centers
```

To find out which element is assigned to which class, the **$cluster** is used.
```{r Cluster}
cluster_alter$cluster
```

The scattering of the data within the groups can be analysed using the **$whithinss**.
```{r Withinss}
cluster_alter$withinss
```

And the size of the classes can be derived using the **$size**. 
```{r Size}
cluster_alter$size
```

## First visual impression
To get an overview of the data the differences among the different elements can be visualized. (using the 1D data)
```{r Visualize distance}
#distance matrix
distance <- dist(alter)

fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

## Quality of the results and best group number analysis
The number of classes, that must be predefined for k-means are crucial for the result.
Its impact is visible when different results are compared for altering group numbers.
(using the 2D data)
```{r group compare}
#using df
#kmeans with predefined groups
cluster_2 <- kmeans(x = df, centers = 2)
cluster_3 <- kmeans(x = df, centers = 3)
cluster_4 <- kmeans(x = df, centers = 4)
cluster_5 <- kmeans(x = df, centers = 5)

#visualization stored as variable
p2 <- fviz_cluster(cluster_2, geom = "point", data = df) + ggtitle("k = 2")
p3 <- fviz_cluster(cluster_3, geom = "point", data = df) + ggtitle("k = 3")
p4 <- fviz_cluster(cluster_4, geom = "point", data = df) + ggtitle("k = 4")
p5 <- fviz_cluster(cluster_5, geom = "point", data = df) + ggtitle("k = 5")

#visualizaion
grid.arrange(p2, p3, p4, p5, nrow = 2)
```

### Find the best group number - 1D data
To visualize the data and check for the cluster results quality, Silhouettes can be used.
They are a graphical representation of the assigning process. It uses two basic elements, the distance matrix and information on which element is assigned to which group.
Building on that, a quality measurement called s(i) is calculated. It includes two measures, the distance of each object to other objects within the same group and the distance to the nearest other group.
The first part can be calculated, using the mean distance of all within group objects. The second part is calculated using the minimum distance, based as well on the mean distance of the  elements of the other groups to the studied element.

#### 1. Create a dataframe with the basic structure of the k-means results data
```{r silhou 1, eval=FALSE}
#number of groups
groupNumber <- cluster$cluster %>% unique() %>% length()
  
#introduce the df
df <- data.frame(elementNumber = NULL, index = NULL, groupNum = NULL)
#fill in the data (element number, index and group number)
for(i in 1:groupNumber){
 #data values for class i
 groupElements <- dat[cluster$cluster == i]
 #index of each element 
 distCalc<- which(dat %in% groupElements)
 df_temp <- data.frame(elementNumber = groupElements, index = distCalc, groupNum = i)
 df <- rbind(df,df_temp)
}
#sort by index
df <- df[order(df$index),]

location = e$cluster #in which group are the elements of "e"
distMat = dm # distance matrix
groupNames = 1:6 #group 1, group 2...
```

#### 2. Calculate the mean distance of every element to the other elements in its group
Add a column named "a" to the dataframe. It contains the distance of the element to each other element in the same group.
```{r silhou 2, eval=FALSE}
df$a <- NA
  
#distance within group calculation
for(i in 1:nrow(df)){
  #all indices of the same group
  allDist <- distMat[df$index[i],]
  indexDist <- df$index[df$groupNum == df$groupNum[i]]
  inGroupDist <- allDist[indexDist]
  inGroupDist <- inGroupDist[inGroupDist != 0]
  #calculate a (in group distance)
  df$a[i] <- mean(inGroupDist)
}
```

#### 3. Calculate the mean distance to each other group and pick the nearest other group
```{r silhou 3, eval=FALSE}
#distance to all other groups df
outGroupDF <- data.frame(index = NULL, ingroup = NULL, outgroup = NULL, b = NULL)
  
#for every element
for(i in 1:nrow(df)){
  groupNames <- unique(df$groupNum)
  groupNames <- sort(groupNames)
  #all other group numbers/names
  groupNames <- groupNames[-df$groupNum[i]]
  #for all other groups
  for(j in 1:length(groupNames)){
    allDist <- dm[df$index[i],]
    index_others <- df$index[df$groupNum == groupNames[j]]
    outGroupDist <- allDist[index_others]
    #mean of the out group distance to group j (for more than 2 groups there are
    #multiple other groups)
    b <- mean(outGroupDist)
    #store them in a df
    outGroupDF_temp <- data.frame(index = df$index[i], ingroup = df$groupNum[i],
                                  outgroup = groupNames, meanDist = b)
    outGroupDF <- rbind(outGroupDF, outGroupDF_temp)
  }
}
#add the nearest group and its b-value
df$nearestGroup <- NA
df$b <- NA
for(i in 1:nrow(df)){
  distToAllOtherGroups <- outGroupDF[outGroupDF$index == i,]
  nearestGroup <- distToAllOtherGroups$outgroup[distToAllOtherGroups$meanDist == min(distToAllOtherGroups$meanDist)]
  df$nearestGroup[i] <- nearestGroup
  df$b[i] <- min(distToAllOtherGroups$meanDist)
}
#set NaN values for a equal b, as s_i turns to 0 in this case
df[is.nan(df$a),4] <- df[is.nan(df$a),6] 
```

#### 4. Calculate s(i)
```{r silhou 4, eval=FALSE}
df$s_i <- NA
  
for(i in 1:nrow(df)){
  df$s_i[i] <- ((df$b[i] - df$a[i])/pmax(df$a[i], df$b[i]))
}
```

#### 5. Mean s(i) over all groups
```{r silhou 5, eval=FALSE}
meanSI <- numeric(0)
for(i in 1:length(unique(df$groupNum))){
  meanSI_temp <- df$s_i[df$groupNum == i] %>% mean()
  meanSI <- c(meanSI, meanSI_temp)
}
```

#### 6.Silhouette coefficient
```{r silhou 6, eval=FALSE}
silcoeff <- mean(df$s_i)
```

#### 7.Data output
```{r silhou 7, eval=FALSE}
#sort the data  by group and  by s_i value
df <- df[order(df$s_i, decreasing = TRUE),]
  
dat <- matrix(0, nrow(df), 3)
dat[,1] <- df$groupNum
dat[,2] <- df$nearestGroup
dat[,3] <- df$s_i

#adjust to strange plot function
#name the columns by index
dimnames(dat) <- list(df$index)

out <- list(dat, meanSI, silcoeff)
return(out)
```

#### Silhouette function and graphical printout 
The presented silhouette function can now be used as a complete unit.
```{r silhouFun}
source(paste0(workingDir, "CloudType_and_Precipitation_Analysis_Repository/FUN_Silhouetten.R"))

sil_params<- silhouette_params(cluster = cluster_alter, distMat = dm, dat = alter)

#print function
plotsilho <- function(silinfo){
  #reversed s(i) values
  S <- rev(silinfo[[1]][, 3])
  #space between groups
  space <- c(0, rev(diff(silinfo[[1]][,1])))
  space[space == -1] <- 1
  #names
  names <- rev(dimnames(silinfo[[1]])[[1]])
  if(!is.character(names)){
    names <- as.character(names)
  }
  barplot(S, space = space, names = names,
          xlab = "Breite der Silhouette", ylab = "",
          xlim = c(min(0, min(S)), 1), horiz = T,
          mgp = c(2.5, 1, 0))
  invisible()
}

plotsilho(sil_params)
```

#### Group number
To find the perfect number of groups, an iteration can be performed, returning the overall s(i) values for the different group numbers. Starting with 2, the number is increased by one until it reaches the number of elements -1. 
```{r groupno, warning=FALSE}
#possible s(i) values are number of elements -2 (starting with 2 groups and ending with group = number of elements -1)
si <- rep(0,length(alter)-2)

#iteration over  oben genannte Klassenzahlen
for(i in 2:(length(alter)-1)){
  e_g <- kmeans(x = matrix(alter,length(alter),1),
         center = matrix(alter[1:i],i,1))
  si[i-1]<-silhouette_params(cluster = e_g, distMat = dm, dat = alter)[[3]]
}
#s(i) value for number of groups= 2,3,4,5
si
```

The quality is defined as follows:

silhouett coefficient | interpretation
----------------------|---------------
0.70 to 1.00          |strong structure
0.51 to 0.70          |moderate structure
0.26 to 0.50          |weak structure
0.00 to 0.25          |no meaningful structure

Another way of identifying the correct group number is calculating the G coefficient using the procedure of Calinski & Harabasz. As larger the G1(K) value as better the result. Although, iterating it over class numbers from 2 to 5 suggest 4 classes which does not hold using the other measurements.
```{r CandH, warning=FALSE}
#process of Calinski & Harabasz for two classes
k <- 2
spT <- sum((length(alter)-1)*var(alter))
e <- kmeans(matrix(alter,ncol=1),matrix(alter[1:k],ncol=1))
spW <- sum(e$withinss)
spB <- spT-spW
(spB/spW)*(length(alter)-k)/(k-1)
```
### Find the best group number - 2D data
#### 1. Ellbow method
One simple way of testing which group number is appropriate is using the *Ellbow Method*. To do so, the total within-cluster sum of square (wss) has to be as small as possible. The implemented method is presented below.
```{r elbow}
set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")
```

The result suggests "breaks" at 2 and at 4 classes. That's where the trend of the data points before and after are disturbed.

#### 2. Average Silhouette method  
Similar to the presented silhouette plot above, the silhouette method uses the distance between the groups and the within variability. As higher the value, as better.
```{r sil}
set.seed(123)

fviz_nbclust(df, kmeans, method = "silhouette")
```

The result suggests "breaks" at 2 and at 4 classes, although 2 classes are the main maxima and 4 classes are the second optimal maxima.

#### 3. Gap statistic method
The gap statistics compares the within-group distance with a distribution that is not clustered. The deviation is presented as gap. As larger the gap value, as more appropriate the group is identified.
```{r gap}
set.seed(123)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
```

As well, the best fit is made with 4 classes.

## Reference
The documentation is based on:

- Handl, A., & Kuhlenkasper, T. (2017). Multivariate Analysemethoden. https://doi.org/10.1007/978-3-662-54754-0 (Kapitel 13 - Clusteranalyse)

Information on how to use more complex data is presented here:

- UC Business Analytics R Programming Guide (2020): K-means Cluster Analysis. https://uc-r.github.io/kmeans_clustering

